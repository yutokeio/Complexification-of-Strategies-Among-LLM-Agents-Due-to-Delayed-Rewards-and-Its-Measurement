# Complexification-of-Strategies-Among-LLM-Agents-Due-to-Delayed-Rewards-and-Its-Measurement
　戦略の条件分岐の数，戦略の話題の数をそれぞれ著者が初期戦略，最終戦略に対して人力で計測した．戦略の条件分岐の数とは，戦略をフローチャート化した時に枝分かれした先のノードの数に相当するものである．フローチャート化する時に，既に通ったノードを再び通る場合は条件分岐の数としてはカウントしないこととする．また，戦略の話題の数とは，戦略の中で言及されている話題のバリエーションのことであり，新しい話題が記載されているのを確認するごとに戦略の話題の数に可算する形で計測する．当初ChatGPTにより複雑性を測る予定であったが，同じ戦略文章に対してChatGPTの出力する複雑性の値に大きくばらつきがあったため，本計測方法は取りやめた．戦略の条件分岐の数の複雑化，戦略の話題の数の複雑化は，それぞれ最終戦略の値から初期戦略の値を引いたものとする．戦略の条件分岐の複雑化と，戦略の話題の数の複雑化の指標がIPD条件，AMPD条件においてどのように変化するのかを比較した．
 まず，戦略の条件分岐の数の計測基準について説明する．基本的に，文字で総利得等を置いた場合の条件分岐の数の数え方を説明する．文字で置いた場合は条件分岐の数としてカウントするが，総利得の計算，相手の前回の選択，前回対戦の利得，利得表においてどのような協調・裏切りの組み合わせの時にどのようなペイオフになるか等を記号化しても，どの戦略でも当たり前に考慮されていることなのでカウントしない．

また，条件分岐の数については，基本的にフローチャート化されて書かれた戦略の方が，そうでない戦略よりも条件分岐の数は大きくなる．以下に戦略の具体例とその解析方法を示す．

表4.1　具体例  
戦略ステップ  
1 初期化  
　ゲームは協力 (C) から開始する．また，相手の協力度スコアを 0.5 に設定し，中立的な立場を示す．  
2 相手の行動評価  
　各ラウンド終了後に相手の行動を評価する．  
•	相手が協力した場合，条件付き協力に進む．  
•	相手が裏切った場合，罰則に進む．  
3 条件付き協力  
　1ラウンド協力 (C) を行う．その後，相手の次の行動を評価する．  
•	相手が協力した場合，相互協力に進む．  
•	相手が裏切った場合，罰則に進む．  
4 相互協力  
　相手が協力を続ける限り協力 (C) を維持する．相互協力の各ラウンドごとに相手の協力度スコアを 0.1 増加させ，最大値は 1 とする．相手の協力からの逸脱を監視する．  
•	相手が協力を継続した場合，相互協力に留まる．  
•	相手が裏切った場合，罰則に進む．  
5 罰則  
　相手の裏切りに対し，1ラウンド裏切り (D) を行い，罰則を与える．相手の協力度スコアを 0.2 減少させ，最小値は 0 とする．その後，相手の次の行動を評価する．  
•	相手が協力した場合，許しに進む．  
•	相手が裏切りを続けた場合，延長罰則に進む．  
6 許し  
　相手の協力度スコアが 0.3 を超えている場合，協力 (C) に戻る．その後，相手の行動を監視する．  
•	相手が協力を継続した場合，相互協力に進む．  
•	相手が再び裏切った場合，罰則に進む．  
7 延長罰則  
　連続したラウンド（例: 2ラウンド）で裏切り (D) を維持し，相手に相互裏切りの結果を認識させる．その後，1回協力 (C) を行い，相手の協力意志を探る．  
•	相手が協力した場合，許しに進む．  
•	相手が裏切りを続けた場合，適応メカニズムに進む．  
8 適応メカニズム  
　相手の協力度を追跡する動的なスコアリングシステムを導入する．協力にはポイントを加算し，裏切りには減点する．相手の行動に基づき，最近の行動により重みを置いてスコアを調整する．  
•	スコアが 0.5 を超えている場合，相互協力に進む．  
•	スコアが 0.2 を下回っている場合，延長罰則に留まる．  
9 利得分析と戦略調整  
　各ラウンド終了後，両者の総利得を評価する．相互協力と交替戦略の総利得を比較する．  
•	相互協力: 総利得は 1 + 1 = 2．  
•	交替戦略: 総利得は -3 + 7 = 4．  
　4 > 2 のため，相手のスコアが 0.5 を下回っている場合，または相手が前ラウンドで裏切った場合，交替戦略を優先する．  
10 交替戦略  
　協力 (C) と裏切り (D) を交替で行い，総利得を最大化する．相手の行動を監視し，必要に応じて戦略を調整する．  

表4.1の戦略文章に対する条件分岐の数のカウント方法について以下に記す．まず，戦略をフローチャート化する．フローチャート化する中で，既に通った戦略ノードは条件分岐としてカウントしないこととすると，以下のようにフローチャートの一覧が示される．既に通った戦略ノードを条件分岐にカウントしないとはどういうことかというと，例えば今回の戦略文章において「初期化→条件付き協力→相互協力→罰則→許し」というフローチャートの箇所において，許しの後に相互協力もしくは罰則のノードに移動するが，これらは既に通ったノードであるため，条件分岐の数としてはこれ以降はカウントしないということである．この規則は，フローチャートが無限ループすることを防ぐために設けている．また，既にカウントした条件分岐の道についても新規にはカウントしない．例えば，下の例では「初期化→条件付き協力→相互協力→罰則→許し，初期化→条件付き協力→相互協力→罰則→延長罰則→許し」の場合，初期化→条件付き協力→相互協力→罰則までは2つ目のフローチャートが1つ目と一致しているため，2つ目の条件分岐の数を数える際に，新たに一致している部分はカウントしない．

初期化→条件付き協力→相互協力→罰則→許し  
初期化→条件付き協力→相互協力→罰則→延長罰則→許し  
初期化→条件付き協力→相互協力→罰則→延長罰則→適応メカニズム  
初期化→罰則→許し→相互協力  
初期化→罰則→延長罰則→許し→相互協力  
初期化→罰則→延長罰則→適応メカニズム→相互協力  

上から順に数えていくと，初期化，条件付き協力，相互協力，罰則，許し，次の行の分岐に進み，延長罰則，許し，さらに次の行の分岐に進み，適応メカニズム，次の行の分岐に進み，罰則，許し，相互協力，次の行の分岐に進み，延長罰則，許し，相互協力，次の行の分岐に進み，適応メカニズム，相互協力となり，計16の分岐としてカウントする．また，最後の交代戦略およびスコアのカウントを加算し，合計18の条件分岐の数となる．

　　また，一つのノードにおけるカウント数が必ずしも1とは限らない．表4.2のノードにおいては，行動として「そのパターンに合わせて戦略を調整します」と「2ラウンド連続で協力を試み，相互協力に戻ることを目指します」の2つがあるため，このノードを通過した際には，条件分岐の数に1ではなく2を加算する．











表4.2　ノードの具体例  
持続的な協力評価と交互パターンの検出  
•	過去 3 ラウンドの相手の行動を評価し，パターンを検出する．  
•	交互パターンが見られる場合，そのパターンから得られる累積利得を相互協力と比較する．  
•	交互パターンが同等以上の利得をもたらす場合，そのパターンに合わせて戦略を調整する．  
•	相互協力の方が高い利得を提供する場合，2 ラウンド連続で協力を試み，相互協力に戻ることを目指す．  

さらに，本論文の主旨でもある，抽象的で曖昧な戦略も条件分岐の数としてカウントする．例えば，表4.3の戦略文は「戦略を見直す」としか記載されていないが，これも条件分岐の一つとして扱う．

表4.3　戦略文の具体例  
4．条件付き分岐  
•	D に切り替えた後，相手も D に切り替えた場合，利得は (-1, -1) である．この状況では，両者が裏切りを続けていることを意味し，長期的なマイナスの結果を避けるために戦略を見直す必要がある．

　戦略がフローチャート化して記載されておらず，戦略が羅列されている形の戦略文も多く存在するが，これらについては戦略の条件分岐の数を計測できないため，戦略の数を上から順に数え，その数を条件分岐の数としてカウントする．フローチャート形式での記載をプロンプト文で指示しているにもかかわらず，これが行われていない場合，戦略が複雑化していないと判断する．その場合，重複する戦略については，同一の戦略としてカウントする例もある．

表4.4　条件分岐の重なりの具体例  
3．タイトル・フォー・タット（Tit-for-Tat）戦略  
•	相手が協力した場合，自分も協力を続ける．  
•	相手が裏切った場合，次のラウンドでは自分も裏切る．  
4．裏切りへの罰則  
•	相互協力のラウンド後に相手が裏切った場合，相手の裏切りへの罰として1ラウンド裏切りを選択する．  
5．許し  
•	相手の裏切りを罰した後，相手が協力した場合，再び協力に戻ることを試みる．  

例えば，表4.4の戦略文章において，Tit-for-Tat戦略は裏切りへの罰則と許しを組み合わせた戦略であるため，これらは条件分岐の数としてまとめて1としてカウントした．

　　また，LLMが一つの戦略の中に互いに矛盾する戦略を記述することがあるが，これについても戦略の曖昧さを表現できていると捉えることができるため，条件分岐の数および話題の数のそれぞれに2を加算する．

　log37のエージェント5に関しては，最終戦略の文章が「5: The gamers' community analyses dynamic multifaceted interactions, in time establishing a better path.」のように一文のみとなっていた．これはシステムエラーによる外れ値として扱い，一つ前の世代の戦略（条件分岐の数11，最終多様性10）を最終戦略として採用した．

次に，話題の数のカウント方法について説明する．基本的に，新しい話題が提供された場合，その戦略の小見出し一つに対して話題の数を1加算する．総利得の計算などはすべての戦略で考慮されているため，条件分岐や話題の多様性にはカウントしない．また，大きい戦略が小さい戦略を包含する場合，小さい戦略はカウントしない．累積利得を変数として置いた場合もカウントしない．互いに矛盾する戦略に関しては，条件分岐および話題の数としてそれぞれ別々にカウントする．相手の前回の選択や直近の利得，利得表における条件を記号化しても，これを条件分岐の数や話題の数としてカウントしない．また，大きい戦略が小さい戦略を包含している場合，小さい戦略は話題の多様性として新たにカウントしない． 

ほとんどのエージェントに含まれている表4.5の基本戦略は，話題の多様性としてまとめて1としてカウントする．ただし，この基本戦略が記述された後にTit-for-Tat戦略などが追加されている場合，Tit-for-Tat戦略と基本戦略は条件分岐の内容として同一であるが，中盤以降の戦略としてTit-for-Tat戦略が記述されていることを考慮し，別の話題としてカウントした．





表4.5　殆どのエージェントに含まれている戦略  
フローチャート戦略  
1．開始: 協力 (C) から始める．  
2．相手が協力した場合: 協力 (C) を選択する．  
3．相手が裏切った場合:  
•	裏切り (D) を選択し，相手の次の行動を監視する．  
•	相手が裏切り後に協力した場合，相互協力を再確立するために協力 (C) を選択する．  
•	相手が再び裏切った場合，裏切り (D) を選択し，協力を時折試みる．  

　基本的に，戦略内に複数の行動が包含されている場合でも，一つの小見出しにつき一つの話題の多様性としてカウントした．表4.6の具体例では，「協調と裏切りを交互に行う」戦略と「2ラウンド連続して裏切りを選択する」戦略という二つの異なる行動が含まれているが，戦略の再評価という観点から，話題の多様性としては1としてカウントした．

表4.6　話題の多様性の数え方の具体例  
6．戦略の再評価  
•	相互協力が最も高い累積利得をもたらす場合，協力 (C) を継続する．  
•	協力と裏切りを交互に行う方が累積利得が高い場合，そのパターンを採用する．  
•	連続的な裏切りが最も高い利得をもたらす場合，2 ラウンド D を選択し，相手の行動を再評価して戦略を調整する．  

　また，利得表の分析に関しては，どの行動パターンが最も期待利得を高めるかまで考慮されているものが望ましい．しかし，単に利得を算出しただけの戦略についても，話題の多様性として1としてカウントした．表4.7に，利得を算出しているが，最適な行動パターンについての考察が記載されていない戦略文の具体例を示す．








表4.7　利得を算出してるが最適な行動パターンの記載無い戦略の具体例  
与えられた利得行列 (C, C): (3, 3)，(C, D): (-3, 7)，(D, C): (7, -3)，(D, D): (-1, -1) に対して最適な戦略を決定するためには，相手が私たちの行動に基づいて戦略を適応させる可能性を考慮しつつ，さまざまな戦略の潜在的な結果を分析する必要がある．  

1．相互協力: 両者が繰り返し協力する場合，両者の合計利得は 3 + 3 = 6 となる．これは，双方が協力を選択する限り，裏切る動機がない安定した状態である．  

2．協力と裏切りの交互戦略: 協力と裏切りを交互に行う場合，1回の協力と1回の裏切りの合計利得は，一方のプレイヤーにとって -3 + 7 = 4，もう一方にとって 7 - 3 = 4 となる．ただし，利得の非対称性により，この交互戦略は単純ではない．  

3．相互裏切り: 両者が繰り返し裏切る場合，両者の合計利得は -1 - 1 = -2 となる．これは，両者にとって最悪の結果であり，相互裏切りが長期的な最適戦略ではないことを示している．  

　さらに，どの行動パターンが最も期待利得を高めるかを把握しているものの，リスクなどの要因を考慮し，一概に最適な行動パターンを決定できないと判断している表4.8のような戦略も，利得表の分析として話題の多様性に1として加算した．  
　
表4.8　期待利得以外にリスクを考慮している戦略  
与えられた利得行列に基づく繰り返しゲームの最適な戦略を構築するには，協力 (C) と裏切り (D) のさまざまな組み合わせによる潜在的な結果を分析する必要がある．  
1．相互協力 (C, C): 両プレイヤーにとっての利得は1となる．  
2．協力と裏切り (C, D) または (D, C): それぞれ，-3と7，または7と-3の利得をもたらす．  
3．相互裏切り (D, D): 両プレイヤーにとっての利得は-1となる．  
与えられた利得を考慮すると，目標は総利得を最大化することである．相互協力の利得は，協力と裏切りを交互に行う場合の潜在的な利得より低いものの，裏切られるリスクが高いため，慎重なアプローチが必要である．  

また，互いに矛盾する二つの戦略文章については，話題の多様性として別々にカウントした．表4.9に具体例を示す．この例では，相手の協力率がちょうど50％の場合の行動として，裏切るべきか協力すべきかの戦略間で矛盾が生じている．しかし，話題の多様性としては，「裏切る場合」と「協力する場合」のそれぞれを1として加算した．  

表4.9　互いに矛盾した戦略の具体例  
•	裏切り (D)  
o	前回の行動が協力 (C) の場合: 裏切り (D)  
o	前回の行動が裏切り (D) の場合:  
	相手の協力率 > 50%: 協力 (C)  
	相手の協力率 ≤ 50%: 裏切り (D)  
o	利得行列が変化した場合，または相手の行動が予測不可能な場合: 利得行列を再分析し，戦略を調整する．  
o	ゲーム継続: ゲームの進行を監視し，必要に応じて戦略を調整する．  

戦略の更新に関する考慮事項  
•	相手の協力率がちょうど50%の場合，相互協力を促進するために協力 (C) を優先する．  

また，条件分岐の数と同様に，重複した戦略が記されている場合は，まとめて話題の多様性を1としてカウントした．表4.4の場合，Tit-for-Tat戦略は裏切りに対する罰則と許しを組み合わせた戦略であるため，これらをまとめて話題の多様性として1とカウントした．  

4.　評価方法　実験2

実験1と同様に話題の数を数えたが，実験1よりも細かく詳細に新規情報をカウントした．新規の情報が少しでも含まれていれば，細かく文章を分割して話題の数を数えた．条件分岐の数を数えない分，より詳細にカウントを行った．実験1と実験2における測定方法の違いを，具体的な例を用いて以下に示す．

表4.10の文章について，実験1では「終了および最終調整」という戦略として1とカウントするが，実験2では，「終盤戦略」という部分の新規性で+1，「直近3ラウンドのうちCを2回以上」という要素で+1，「それ以外の場合」で+1，「相手の直近10ラウンドの協力比率が60％以上」という要素で+1とする．行動に関しては，新規性のある行動が含まれていないため加算しない．したがって，合計3としてカウントする．



表4.10　実験1と実験2の測定方法の違いの具体例  
終了および最終調整（Termination and Final Optimization）  
•	所定のラウンド数または報酬のマイルストーンに達したら，最終戦略の評価を実施する．  
•	相手の直近3ラウンドの行動に基づいて最終的な行動を決定する:  
○ 直近3ラウンドのうち，相手がCを2回以上選択した場合，Cを選択．  
○ それ以外の場合，相手の直近10ラウンドの協力比率が60%以上であればCを選択し，それ以外はDを選択．  

　実験1と同様に，殆どのエージェントに含まれている表4.11の基本的な戦略は，話題の多様性としては1としてカウントした．

表4.11　基本的な戦略  
フローチャート戦略  
1．開始: 協力 (C) から始める．  
2．相手が協力した場合: 協力 (C) を選択する．  
3．相手が裏切った場合:  
•	裏切り (D) を選択し，相手の次の行動を監視する．  
•	相手が裏切り後に協力した場合，相互協力を再確立するために協力 (C) を選択する．  
•	相手が再び裏切った場合，裏切り (D) を選択し，協力を時折試みる．  

実験1と同様に，総利得の計算などはどの戦略においても一般的に考慮されるため，条件分岐の数や話題の多様性にはカウントしない．大きな戦略が小さな戦略を包含する場合，小さな戦略の方はカウントしない．累積利得を何らかの変数として設定した場合もカウントしないが，全体の平均報酬などを変数として設定したものについてはカウントした．また，互いに矛盾する戦略については，条件分岐および話題の数として別々にカウントした．相手の前回の選択や直近の利得，利得表を基に，どのような行動がどの利得をもたらすかを記号化したものについては，条件分岐の数や話題の多様性にはカウントしない．大きな戦略が小さな戦略を包含する場合，小さな戦略は話題の多様性として新たにカウントしない．

　「慎重な裏切り戦略を採用」や「協力の維持または強化を検討」のような抽象的な表現も，多様性としてそれぞれ1としてカウントした．

表4.12　場合分けの場合の数え方  
条件分岐（Conditional Branch）  
1．相手の欠陥率，連続欠陥数，および自分の最後の行動を評価:  
•	もし opponent_defect_rate > opponent_defect_threshold または consecutive_defects > 2 の場合，搾取を防ぐために次の行動は欠陥（D）を選択．  
•	もし前回の行動が協力（C）で相手が欠陥（D）の場合，報復として次の行動は欠陥（D）．  
•	もし相手の行動が協力（C）で自分の前回の行動が欠陥（D）の場合，協力（C）を選択し，協力を再構築．  
•	もし mutual_cooperation_count > 2 の場合，協力（C）を優先し，相互報酬を最大化．  
•	それ以外の場合，recent_move_history を分析:  
o	直近5ラウンドで相手が協力よりも多く欠陥していた場合，欠陥（D）を選択．  
o	相手が欠陥よりも多く協力していた場合，協力（C）を選択．  
o	どちらも同数の場合，または初回評価時は相手の行動を模倣．  

また，表4.12のような例では，opponent_defect_rate > opponent_defect_threshold で+1，consecutive_defects > 2 で+1，mutual_cooperation_count > 2 で+1，「それ以外の場合」で+1，「直近5ラウンド」で+1，「相手が協力よりも多く欠陥していた場合」で+1，「相手が欠陥よりも多く協力していた場合」で+1，「どちらも同数の場合」で+1，「相手の行動を模倣」で+1といったようにカウントした．

表4.13　適応的な閾値設定  
　　8．適応的な閾値設定  
•	相手の行動に基づいて協力度の閾値を調整．  
o	協力度が上昇した場合，現在の閾値を維持．  
o	協力度が低下した場合，閾値を65%に引き上げ．  

また，上記のような例では，「協力度が上昇」で+1，「協力度が低下」で+1，「閾値を引き上げ」で+1としてカウントした．






表4.14　矛盾の例  
協力対応  
1．協力（C）を選択し，相互協力を維持．  
2．相手の次の行動を監視:  
•	相手が協力（C）した場合，協力（C）を継続し，相互協力トラッカーをリセット．  
•	相手が裏切り（D）した場合，**「裏切り対応」**の分岐に進む．  

　さらに，表4.14のような例では，「相互協力トラッカー」は本来，相互協力の発生をカウントする機能を持つべきものであるにもかかわらず，相互協力が起きるたびにリセットされるという矛盾が見られた．しかし，新規性が認められるため，+1としてカウントした．
実験1における初期戦略とそれに基づいたエージェントの行動履歴を分析すると，例えばlog26のエージェント1の初期戦略は表5.4のようである．この戦略では，直前の相手の行動を模倣することが戦略として挙げられているが，実際のエージェント1の1世代目での行動を見ると，図5.1のように，相手が裏切りをしているにもかかわらず幾度も協調を選択している．このように，エージェントが戦略通りに動いていないため，LLMが戦略を改良する際に，元の戦略の結果としてどのような結果が得られたかの情報が不足しており，適切な戦略改良が困難となる可能性が考えられる．

表5.4 log26のエージェント1の初期戦略  
与えられたペイオフマトリックスに基づく反復ゲームの最適な戦略を作成するために、協力（C）と裏切り（D）のさまざまな組み合わせの潜在的な結果を分析する必要があります。  
ペイオフマトリックス：  
•	(C, C): (1, 1)  
•	(C, D): (-3, 7)  
•	(D, C): (7, -3)  
•	(D, D): (-1, -1)  
各戦略のペイオフ  
1.	**相互協力（C, C）**は、両プレイヤーにとって1のペイオフをもたらします。  
2.	**片方が協力し、もう片方が裏切る（C, D または D, C）**と、それぞれペイオフは(-3, 7)または(7, -3)となります。  
3.	**相互裏切り（D, D）**は、両者ともに-1のペイオフとなります。  
戦略の考察
このペイオフを考慮すると、総合的なペイオフを最大化することが目標となります。相互協力によるペイオフは安定しているものの、協力と裏切りを交互に行うことでより高いペイオフを得る可能性があります。しかし、相手が裏切るリスクも高いため、慎重なアプローチが必要です。  
最適な戦略としては、**協力から始め、相手の前回の行動を模倣する（Tit-for-Tat戦略）**ことが有効です。この戦略により、協力を促進しつつ、搾取されることを防ぐことができます。  
フローチャート戦略  
1.	協力（C）から開始する。  
2.	相手の直前の行動を観察する。  
o	相手が協力（C）した場合、自分も協力（C）を選択する。  
o	相手が裏切り（D）した場合、自分も裏切り（D）を選択する。  
3.	各ラウンドでステップ2を繰り返す。  
このアプローチにより、相互協力の可能性を確保しつつ、適度な安定したペイオフを得ることができます。また、相手が裏切りを選択した場合でも、すぐに対応できる防御策となります。  

 Generation 1  
  Match: 1 vs 2 (20 turns)  
    Agent 1 chose C | Agent 2 chose D | Payoff: -3 vs 7  
    Agent 1 chose D | Agent 2 chose D | Payoff: -1 vs -1  
    Agent 1 chose D | Agent 2 chose D | Payoff: -1 vs -1  
    Agent 1 chose C | Agent 2 chose D | Payoff: -3 vs 7  
    Agent 1 chose C | Agent 2 chose D | Payoff: -3 vs 7  
    Agent 1 chose D | Agent 2 chose D | Payoff: -1 vs -1  
    Agent 1 chose C | Agent 2 chose D | Payoff: -3 vs 7  
    Agent 1 chose C | Agent 2 chose D | Payoff: -3 vs 7  
    Agent 1 chose D | Agent 2 chose D | Payoff: -1 vs -1  
    Agent 1 chose D | Agent 2 chose D | Payoff: -1 vs -1  
    Agent 1 chose C | Agent 2 chose D | Payoff: -3 vs 7  
    Agent 1 chose C | Agent 2 chose D | Payoff: -3 vs 7  
    Agent 1 chose C | Agent 2 chose D | Payoff: -3 vs 7  
    Agent 1 chose D | Agent 2 chose D | Payoff: -1 vs -1  
    Agent 1 chose D | Agent 2 chose D | Payoff: -1 vs -1  
    Agent 1 chose C | Agent 2 chose C | Payoff: 1 vs 1  
    Agent 1 chose C | Agent 2 chose C | Payoff: 1 vs 1  
図5.1　log26のエージェント1の1世代目における行動  

　また，エージェントが人間のように矛盾を抱えた二つの戦略を内在している状態は，以下の例のように再現されることが確認された．例えば，以下の例では，「直近10ラウンドの相手の協力率が30％未満の場合，しっぺ返し戦略を適用」と記載されている．この場合，相手の協力率が30％未満であり，かつ相手が三回連続で裏切った後に協調してきた場合，しっぺ返し戦略に従うと協調を選択することになるが，連続的な裏切り戦略に従うと裏切りを選択することになり，矛盾が生じる．  
表5.5　実験1における人間のように矛盾を抱えた二つの戦略  
しっぺ返し（Tit-for-Tat）戦略の適用：  
直近10ラウンドの相手の協力率が30％未満の場合、しっぺ返し戦略を適用し、相手の行動に応じて協力と裏切りを選択する。  

連続的な裏切り：相手が3回連続で裏切った場合、次の2ラウンドは裏切り（D）を選択し、相手の行動への不満を示す。  


5.3.2　実験2

実験2において，初期戦略は表5.6の通りであるため，5ラウンドごとに協調もしくは裏切りをランダムに選択するフェーズを除けば，相手が協調した場合，次のターンで自分も協調するという行動パターンになるはずである．しかし，図5.2のように，最初の2，3ターンではこの通りに動作するものの，その後，相手が前回のラウンドで協調を選択したにもかかわらず裏切りを行い，その後相互裏切りを繰り返すケースが目立った．LLMが戦略通りに協調・裏切りを決定しないため，LLMが戦略を改良する際に，元の戦略の結果が適切に反映されず，戦略の改良が困難となる可能性が考えられる．



表5.6　実験2の初期戦略  
開始: 協力（C）から始める。  
相手が協力した場合: 協力（C）を選択する。  
相手が裏切った場合:  
•	裏切り（D）を選択し、相手の次の動きを監視する。  
•	相手が裏切りの後に協力した場合、相互協力を再確立するために協力（C）を選択する。  
•	相手が再び裏切った場合、裏切り（D）を選択し、時折協力を試みる。  
5ラウンドごとに、協力または裏切りをランダムに選択する。  

 Match: 1 vs 5 (27 turns)  
Agent 1 chose C | Agent 5 chose D | Payoff: -3 vs 7  
Agent 1 chose D | Agent 5 chose C | Payoff: 7 vs -3  
Agent 1 chose C | Agent 5 chose D | Payoff: -3 vs 7  
Agent 1 chose D | Agent 5 chose D | Payoff: -1 vs -1  
Agent 1 chose D | Agent 5 chose D | Payoff: -1 vs -1  
  
図5.2　実験2における1世代目の対戦結果の一例


　実験2においては，エージェントに対して利得表を与えず，対戦結果から利得表を読み取るように促した．しかし，エージェントが利得表を誤って推測する事例がいくつか発生した．例えば，「自身が裏切り【D】，相手が協調【C】を選択し，ペイオフが5の場合，次のラウンドでは【C】を選択し，相手の協力的な姿勢を活かしつつ，相互協力へと誘導する．」という戦略文では，自身が裏切り，相手が協調した際の自身の利得を7ではなく5と誤って認識しているケースが確認された．このことから，LLMは全ての対戦履歴を詳細に把握できていない可能性が考えられる．例えば，log55のエージェント2は，図5.3のような誤った利得表を提示したが，log55はAMPD条件のため，実際の利得表は【(C, C): (1, 1)，(C, D): (-3, 7)，(D, C): (7, -3)，(D, D): (-1, -1)】であり，提示された利得表はすべてのペイオフ値が異なっている．LLMには戦略改良や協調・裏切りの決定のために過去の対戦履歴を全て与えているが，それらを誤って解釈していることが明らかとなった．この原因として，LLMに送信する文章量が長くなりすぎたため，LLMがすべての情報を適切に理解できなかった可能性が考えられる．そのため，LLMモデルの性能をより高いものに変更するか，LLMに送信する対戦履歴を直近の一世代のみとするなどの改良が必要であると考察される．


4. Define a payoff matrix:  
   - C/C: 3/3  
   - C/D: 0/5  
   - D/C: 5/0  
   - D/D: 1/1  
  
図5.3　log55のエージェント2が示した利得表

また，エージェントが人間のように矛盾を抱えた二つの戦略を内在している状態は，表5.7の例のように再現することができた．例えば，以下の例では，”5ラウンドごとの再評価で裏切りが多い場合，1ラウンドの裏切り(D)”とする一方で，”直近2ラウンドの報酬が負であれば協力(C)”とする二つの条件が重なった場合，意思決定に矛盾が生じてしまう．このように，LLMは人間のように戦略間で矛盾が生じる状況を再現できるが，最終的にLLMは適切な意思決定を行うことができる．

表5.7　実験2における人間のように矛盾を抱えた二つの戦略  
適応的な再評価  
•	5ラウンドごとに、過去5ラウンドの結果に基づいて戦略を再評価する：  
o	協力の回数が裏切りの回数を上回る場合、主戦略を維持し、相互利益を強化する。  
o	裏切りの回数が協力を上回る場合、1ラウンドの裏切り（D）を実施し、その後相手の反応を再評価する。  

最適化の改良  
•	直近2ラウンドの合計報酬が負であり、両者が前回裏切りを選択した場合、次のラウンドでは協力（C）を試み、相互利益の再確立を図る。

