# -*- coding: utf-8 -*-
"""example3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12bqCQefxsyfujS2fV86RQh6laLefrceD
"""

#グラフ機能や、履歴を三世代前までにする機能追加
import random
import requests
import openai


import urllib3
import warnings
number="chatGPTlog16"
MODEL = "o3-mini"


API_KEY = ""

# ========= ここからグラフ描画ライブラリの追加部分 =========
import matplotlib
matplotlib.use('Agg')  # サーバ上などディスプレイの無い環境でも出力できるよう設定
import matplotlib.pyplot as plt
# ========= ここまでグラフ描画ライブラリの追加部分 =========

# InsecureRequestWarningを無視
warnings.simplefilter('ignore', urllib3.exceptions.InsecureRequestWarning)


# 利得表の設定
payoff_matrix = {
    "(C, C)": (3, 3),
    "(C, D)": (-3, 7),
    "(D, C)": (7, -3),
    "(D, D)": (-1, -1)
}


policies = {
    agent: f"""
    You are engaged in a game-theoretic competition. There are Agent1, Agent2, and Agent3, and they will compete in a round-robin tournament. You are Agent{agent}.

    """ + (
        """
        Regardless of whether the opponent chooses cooperation or defection, I will continue to cooperate forever.
        """ if agent == 1 else """
        Choose cooperation at first, and after that, mimic the opponent's previous move.
        """ if agent == 2 else """
        Start with cooperation (C),  and after that, alternate between cooperation (C) and defection (D). This strategy follows a fixed pattern regardless of the opponent's actions.
        """
    )
    for agent in range(1, 4)
}

def decide_action(agent_name, policy, history):

    client = openai.OpenAI(api_key=API_KEY)  # ✅ 新しいクライアントオブジェクトを作成

    request_data = {
        "model": MODEL,
        "messages": history + [
            {"role": "user", "content": f"Agent {agent_name}, based on the policy: {policy} and considering the history of past interactions, respond with exactly one letter: 'C' for cooperate or 'D' for defect. Provide no explanation or additional text."},
        ]
    }
    while True:
        response = client.chat.completions.create(**request_data)  # ✅ 新しい書き方
        choice_agent = response.choices[0].message.content.strip()  # ✅ 新しいレスポンス構造に対応
        if choice_agent in {"C", "D"}:
            break

    return choice_agent


def refine_policy(agent_name, previous_policy, history):
    client = openai.OpenAI(api_key=API_KEY)  # ✅ 新しいクライアントオブジェクトを作成

    request_data = {
        "model": MODEL,
        "messages": history + [
            {
                "role": "user",
                "content": (
                    f"You are engaged in a game-theoretic competition.There are Agent1, Agent2, Agent3, Agent4, and Agent5, and they will compete in a round-robin tournament. You are Agent{agent_name}."
                    f"Agent{agent_name},you are a highly knowledgeable expert in game theory, well-versed in analyzing strategic interactions and consider a specific strategy in repeated games. You are participating in a repeated game with an agent. "
                    " Your task is to create a specific strategy that maximizes your total payoff over the course of the game."
                    "your task is to produce a **fully self-contained** updated strategy. "
                    "Below is your current (previous) strategy:\n"
                    f"{previous_policy}\n\n"
                    "Based on the outcomes of all past interactions (see system messages above),your current strategy and the analysis of the payoff matrix, "
                    "you must **integrate all the good parts** of the current policy while **improving or removing** suboptimal aspects. "
                    "Write the result as **one complete, standalone description** of the new strategy. "
                    "Explore various strategic options. "
                    "Strive to balance short-term gains with long-term benefits."
                    "Write a strategy in the form of a flowchart with conditional branches, so that cooperation or defection can be automatically chosen by following it."
                    "A reader should be able to understand this entire strategy **without any additional context**.\n\n"
                    "Do not use conversational tone, do not address any partner directly, and avoid phrases like 'let's hope'. "
                    "Present the final strategy in a clear, cohesive form, with all necessary details included. "
                    "Let's think step by step."
                )
            }
        ]
    }
    response = client.chat.completions.create(**request_data)  # ✅ 新しい書き方

    return response.choices[0].message.content.strip()



# 総当たり戦の実行
def run_round_robin(agents, generations, num_turns=10):
    histories = {agent: {opponent: [] for opponent in agents if opponent != agent} for agent in agents}
    total_payoffs = {agent: 0 for agent in agents}
    total_turns = {agent: 0 for agent in agents}
    print(policies)

    # ========= 追加: 世代ごとの合計協調率を記録するリスト =========
    cooperation_rates_over_generations = []
    # ============================================================

    for gen in range(generations):
        print(f"Generation {gen + 1}")

        # ========= 追加: 今世代の協調数・行動数、エージェント別の協調数をカウント =========
        this_gen_cooperation_count = 0  # 全エージェント合計の「C」回数
        this_gen_action_count = 0      # 全エージェント合計の行動数
        agent_coop_count_current_gen = {agent: 0 for agent in agents}   # エージェントごと
        agent_action_count_current_gen = {agent: 0 for agent in agents} # エージェントごと
        # =========================================================================

        for i, agent1 in enumerate(agents):
            for j, agent2 in enumerate(agents):
                if i >= j:
                    continue

                print(f"  Match: {agent1} vs {agent2} ({num_turns} turns)")

                for turn in range(num_turns):
                    action1 = decide_action(agent1, policies[agent1], histories[agent1][agent2])
                    action2 = decide_action(agent2, policies[agent2], histories[agent2][agent1])

                    payoff = payoff_matrix.get(f"({action1}, {action2})", (0, 0))
                    total_payoffs[agent1] += payoff[0]
                    total_payoffs[agent2] += payoff[1]

                    total_turns[agent1] += 1
                    total_turns[agent2] += 1

                    histories[agent1][agent2].append({
                        "role": "system",
                        "content": f"Turn{turn + 2} you chose {action1}  Agent{agent2} chose {action2} payoff: {payoff[0]}",
                        "generation": gen
                    })
                    histories[agent2][agent1].append({
                        "role": "system",
                        "content": f"Turn{turn + 2} you chose {action2}  Agent{agent1} chose {action1} payoff: {payoff[1]}",
                        "generation": gen
                    })

                    print(f"  Agent {agent1} chose {action1} | Agent {agent2} chose {action2} | Payoff: {payoff[0]} vs {payoff[1]}")

                    # ========= 追加: 行動をカウントして協調率計算に利用 =========
                    agent_action_count_current_gen[agent1] += 1
                    agent_action_count_current_gen[agent2] += 1
                    if action1 == "C":
                        agent_coop_count_current_gen[agent1] += 1
                        this_gen_cooperation_count += 1
                    if action2 == "C":
                        agent_coop_count_current_gen[agent2] += 1
                        this_gen_cooperation_count += 1
                    this_gen_action_count += 2
                    # =======================================================

        for agent in agents:
            all_histories = sum(histories[agent].values(), [])
            policies[agent] = refine_policy(agent, policies[agent], all_histories)

        # ── ここで現在の世代以外の履歴を除去 ──
        for agent in agents:
          for opponent in histories[agent]:  # 各対戦相手のリストをフィルタリング
              histories[agent][opponent] = [
                 h for h in histories[agent][opponent]
                 if h.get("generation", -1) >= gen
              ]
        # ───────────────────────────────────────────────

        print(policies)
        for agent in agents:
            print(f"{agent}：", total_payoffs[agent] / total_turns[agent])

        # ========= 追加: 今世代の合計協調率をリストに保存 =========
        if this_gen_action_count > 0:
            generation_coop_rate = this_gen_cooperation_count / this_gen_action_count
        else:
            generation_coop_rate = 0
        cooperation_rates_over_generations.append(generation_coop_rate)
        # =====================================================

        # ========= 追加: 世代数が5の倍数のとき、各エージェント協調率をヒストグラムで出力 =========
        if (gen + 1) % 5 == 0:
            # 各エージェントの協調率を算出
            agent_coop_rates = []
            for agent in agents:
                if agent_action_count_current_gen[agent] > 0:
                    r = agent_coop_count_current_gen[agent] / agent_action_count_current_gen[agent]
                else:
                    r = 0
                agent_coop_rates.append(r)

            # ヒストグラムの描画
            plt.figure()
            plt.bar(range(len(agents)), agent_coop_rates, tick_label=agents)
            plt.ylim(0, 1)
            plt.xlabel('Agent')
            plt.ylabel('Cooperation Rate')
            plt.title(f'Cooperation Rates at Generation {gen+1}')
            plt.savefig(f'{str(number)}cooperation_hist_{gen+1}.png')
            plt.close()
        # ========================================================================================

    average_payoffs = {
        agent: (total_payoffs[agent] / total_turns[agent] if total_turns[agent] > 0 else 0)
        for agent in agents
    }

    # ========= 追加: 全世代を通した合計協調率の推移を折れ線グラフで出力 =========
    plt.figure()
    plt.plot(range(1, generations + 1), cooperation_rates_over_generations, marker='o')
    plt.xlabel('Generation')
    plt.ylabel('Cooperation Rate')
    plt.title('Cooperation Rate Over Generations')
    plt.savefig(f'{str(number)}cooperation_rate_over_generations.png')
    plt.close()
    # =========================================================================

    return average_payoffs, policies

# エージェントの設定と実行
agents = [i for i in range(1, 4)]########################################################################
generations = 20
payoffs, final_policies = run_round_robin(agents, generations)

# 結果を表示
print("Final Payoffs:")
for agent, payoff in payoffs.items():
    print(f"{agent}: {payoff}")

print("\nFinal Policies:")
for agent, policy in final_policies.items():
    print(f"{agent}:\n{policy}")